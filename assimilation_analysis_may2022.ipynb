{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "clear_output(wait=True)\n",
    "print(\"statsmodels installed\")\n",
    "!pip install sklearn\n",
    "clear_output(wait=True)\n",
    "print(\"sklearn installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "# Import these costom functions from the toolbox\n",
    "sys.path.insert(1, './tools/')\n",
    "import metrics\n",
    "import signatures\n",
    "import analysis_tools\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK SETUP.\n",
    "# CHOSE OPTIONS, PATHS, DATE RANGES, ETC. HERE IN THIS CELL\n",
    "\n",
    "# Options for what to calculate\n",
    "# Calulations will be saved to pkl\n",
    "# If False, will load in pkl files\n",
    "######################################\n",
    "calculate_metric_dictionary = True #\n",
    "calculate_flow_categories = True   #\n",
    "calculate_metric_matrix = True     #\n",
    "######################################\n",
    "\n",
    "val_start='10/01/1989'\n",
    "val_end='09/30/1999'\n",
    "date_range = pd.date_range(start=val_start, end=val_end)\n",
    "\n",
    "# Specifics about the analysis to use. \n",
    "all_metrics = ['NSE', 'Alpha-NSE', 'Pearson-r', 'Beta-NSE', 'FHV', 'FMS', 'FLV', 'Peak-Timing']\n",
    "use_metrics = ['NSE', 'Beta-NSE', 'Peak-Timing']\n",
    "metric_names = ['Nash-Sutcliffe Efficiency', 'Total bias', 'Peak timing error']\n",
    "short_metric_names = ['NSE', 'Bias', 'Timing']\n",
    "use_metric_names = ['Nash-Sutcliffe Efficiency', 'Total bias', 'Peak timing error']\n",
    "use_metric_locs = [0,3,7]\n",
    "# Plotting parameters for the metrics defined above.\n",
    "disp_bounds = [(-0,1), (-0.2,0.1), (0,2)]\n",
    "diff_bounds = [1, 0.2, 2]\n",
    "optimal = [1, 0, 0]\n",
    "disp_colors = {'base_model':'PRGn', 'lagged_streamflow_both':'PRGn'} \n",
    "\n",
    "# local directory with all data\n",
    "# not included in github\n",
    "data_dir = \"./data/\"\n",
    "run_dir = \"/home/NearingLab/projects/jmframe/lstm-da/runs/time_split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Open the attributes set up to do regression\n",
    "    openthis = data_dir+'camels_attributes_v2.0/camels_attributes_v2.0_Regression.csv'\n",
    "    attributes = pd.read_csv(openthis, sep=',', index_col='gauge_id')\n",
    "else: # Open a slightly more extrnsive data set.\n",
    "    openthis = data_dir+'camels_attributes_v2.0/camels_attributes_v2.0.csv'\n",
    "    attributes = pd.read_csv(openthis, sep=';', index_col='gauge_id')\n",
    "\n",
    "# Catchment attributes and hydrologic signatures that are not useful\n",
    "drop_these = ['high_prec_timing','root_depth_50', 'root_depth_99','zero_q_freq','water_frac','organic_frac',\n",
    "              'low_prec_timing', 'geol_1st_class','dom_land_cover','other_frac']\n",
    "    \n",
    "# Add the basin ID as a 8 element string with a leading zero if neccessary\n",
    "basin_id_str = []\n",
    "for a in attributes.index.values:\n",
    "    basin_id_str.append(str(a).zfill(8))\n",
    "attributes['basin_id_str'] = basin_id_str\n",
    "\n",
    "# These are bad for the regression analysis.\n",
    "attributes = attributes.drop(drop_these, axis=1)\n",
    "df = copy.deepcopy(attributes)\n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hydrologic units for each basin.\n",
    "with open(data_dir + 'usgs_site_info.csv', 'r') as f:\n",
    "    usgs_sites = pd.read_csv(f, skiprows=24, index_col='site_no')\n",
    "usgs_idx_int = []\n",
    "for idx in usgs_sites.index.values:\n",
    "    usgs_idx_int.append(int(idx))\n",
    "usgs_sites.reindex(usgs_idx_int)\n",
    "usgs_sites = usgs_sites.reindex(usgs_idx_int)\n",
    "basin_hydro_unit = []\n",
    "for b in attributes.basin_id_str.values:\n",
    "    huc_cd = usgs_sites.loc[int(b),'huc_cd']\n",
    "    hu = '{:08d}'.format(huc_cd)\n",
    "    basin_hydro_unit.append(hu[0:2])\n",
    "attributes['basin_hydro_unit'] = basin_hydro_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "sim_dir = \"simulation/simulation_seed_0_1304_224615/test/model_epoch030/\"\n",
    "#da_dir = \"assimilation/assimilation_seed_0_holdout_0.0_lead_2/test/model_epoch030/\"\n",
    "da_dir = \"assimilation/assimilation_seed_0_holdout_0.25_lead_10/test/model_epoch030/\"\n",
    "ar_dir = \"autoregression/autoregression_lead_1_train_holdout_0.5_test_holdout_0.0_seed_0/test/model_epoch030/\"\n",
    "run_dir_dict = {\"sim\":sim_dir, \"da\":da_dir, \"ar\":ar_dir}\n",
    "\n",
    "for run_type in run_dir_dict.keys():\n",
    "    with open(run_dir + run_dir_dict[run_type] + \"test_results.p\", \"rb\") as fb:\n",
    "        results[run_type] = pkl.load(fb)\n",
    "\n",
    "# with open(run_dir + da_dir + \"test_results.p\", \"rb\") as fb:\n",
    "#     results['da'] = pkl.load(fb)\n",
    "\n",
    "# with open(run_dir + ar_dir + \"test_results.p\", \"rb\") as fb:\n",
    "#     results['ar'] = pkl.load(fb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_attributes = ['elev_mean','slope_mean','area_gages2',\n",
    "'frac_forest','lai_max','lai_diff','gvf_max',\n",
    "'gvf_diff','soil_depth_pelletier','soil_depth_statsgo','soil_porosity',\n",
    "'soil_conductivity','max_water_content','sand_frac','silt_frac','clay_frac',\n",
    "'carbonate_rocks_frac','geol_permeability','p_mean','pet_mean','aridity',\n",
    "'frac_snow','high_prec_freq','high_prec_dur','low_prec_freq','low_prec_dur']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_list = results['sim'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lats = []\n",
    "plot_lons = []\n",
    "for i, b in enumerate(basin_list):\n",
    "    plot_lats.append(attributes.loc[int(b),'gauge_lat'])\n",
    "    plot_lons.append(attributes.loc[int(b),'gauge_lon'])\n",
    "plot_lats = np.array(plot_lats)\n",
    "plot_lons = np.array(plot_lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('basin_id_str').loc[basin_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nse_sim'] = np.nan\n",
    "df['nse_ar'] = np.nan\n",
    "df['nse_da'] = np.nan\n",
    "df['nse_ar-sim'] = np.nan\n",
    "df['nse_da-sim'] = np.nan\n",
    "df['nse_da-ar'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for basin in basin_list:\n",
    "#     df['nse_sim'] = metrics.nse(results['sim'][basin]['1D']['xr']['QObs(mm/d)_obs'], \n",
    "#                                 results['sim'][basin]['1D']['xr']['QObs(mm/d)_sim'])\n",
    "    df.loc[basin,'nse_sim'] = results['sim'][basin]['1D']['NSE']\n",
    "    df.loc[basin,'nse_ar'] = results['ar'][basin]['1D']['NSE']\n",
    "    df.loc[basin,'nse_da'] = results['da'][basin]['1D']['NSE']\n",
    "    df.loc[basin,'nse_ar-sim'] = results['ar'][basin]['1D']['NSE'] - results['sim'][basin]['1D']['NSE']\n",
    "    df.loc[basin,'nse_da-sim'] = results['da'][basin]['1D']['NSE'] - results['sim'][basin]['1D']['NSE']\n",
    "    df.loc[basin,'nse_da-ar'] = results['da'][basin]['1D']['NSE'] - results['ar'][basin]['1D']['NSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_mean</th>\n",
       "      <th>pet_mean</th>\n",
       "      <th>p_seasonality</th>\n",
       "      <th>frac_snow</th>\n",
       "      <th>aridity</th>\n",
       "      <th>high_prec_freq</th>\n",
       "      <th>high_prec_dur</th>\n",
       "      <th>low_prec_freq</th>\n",
       "      <th>low_prec_dur</th>\n",
       "      <th>glim_1st_class_frac</th>\n",
       "      <th>...</th>\n",
       "      <th>slope_mean</th>\n",
       "      <th>area_gages2</th>\n",
       "      <th>area_geospa_fabric</th>\n",
       "      <th>baseflow_index</th>\n",
       "      <th>nse_sim</th>\n",
       "      <th>nse_ar</th>\n",
       "      <th>nse_da</th>\n",
       "      <th>nse_ar-sim</th>\n",
       "      <th>nse_da-sim</th>\n",
       "      <th>nse_da-ar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_id_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01022500</th>\n",
       "      <td>3.608126</td>\n",
       "      <td>2.119256</td>\n",
       "      <td>-0.114530</td>\n",
       "      <td>0.245259</td>\n",
       "      <td>0.587356</td>\n",
       "      <td>20.55</td>\n",
       "      <td>1.205279</td>\n",
       "      <td>233.65</td>\n",
       "      <td>3.662226</td>\n",
       "      <td>0.590658</td>\n",
       "      <td>...</td>\n",
       "      <td>17.79072</td>\n",
       "      <td>573.60</td>\n",
       "      <td>620.38</td>\n",
       "      <td>0.554478</td>\n",
       "      <td>0.858931</td>\n",
       "      <td>0.960366</td>\n",
       "      <td>0.858931</td>\n",
       "      <td>0.101435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.101435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01031500</th>\n",
       "      <td>3.522957</td>\n",
       "      <td>2.071324</td>\n",
       "      <td>0.104091</td>\n",
       "      <td>0.291836</td>\n",
       "      <td>0.587950</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1.148936</td>\n",
       "      <td>227.35</td>\n",
       "      <td>3.473644</td>\n",
       "      <td>0.448928</td>\n",
       "      <td>...</td>\n",
       "      <td>29.56035</td>\n",
       "      <td>769.05</td>\n",
       "      <td>766.53</td>\n",
       "      <td>0.445091</td>\n",
       "      <td>0.896980</td>\n",
       "      <td>0.950614</td>\n",
       "      <td>0.896980</td>\n",
       "      <td>0.053634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.053634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01047000</th>\n",
       "      <td>3.323146</td>\n",
       "      <td>2.090024</td>\n",
       "      <td>0.147776</td>\n",
       "      <td>0.280118</td>\n",
       "      <td>0.628929</td>\n",
       "      <td>20.10</td>\n",
       "      <td>1.165217</td>\n",
       "      <td>235.90</td>\n",
       "      <td>3.691706</td>\n",
       "      <td>0.308488</td>\n",
       "      <td>...</td>\n",
       "      <td>49.92122</td>\n",
       "      <td>909.10</td>\n",
       "      <td>904.94</td>\n",
       "      <td>0.473465</td>\n",
       "      <td>0.859059</td>\n",
       "      <td>0.919627</td>\n",
       "      <td>0.859059</td>\n",
       "      <td>0.060569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.060569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01052500</th>\n",
       "      <td>3.730858</td>\n",
       "      <td>2.096423</td>\n",
       "      <td>0.152097</td>\n",
       "      <td>0.352698</td>\n",
       "      <td>0.561914</td>\n",
       "      <td>13.50</td>\n",
       "      <td>1.129707</td>\n",
       "      <td>193.50</td>\n",
       "      <td>2.896707</td>\n",
       "      <td>0.497458</td>\n",
       "      <td>...</td>\n",
       "      <td>60.05183</td>\n",
       "      <td>383.82</td>\n",
       "      <td>396.10</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.871757</td>\n",
       "      <td>0.928358</td>\n",
       "      <td>0.871757</td>\n",
       "      <td>0.056601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.056601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01054200</th>\n",
       "      <td>4.067132</td>\n",
       "      <td>2.128355</td>\n",
       "      <td>0.104960</td>\n",
       "      <td>0.299642</td>\n",
       "      <td>0.523306</td>\n",
       "      <td>17.50</td>\n",
       "      <td>1.194539</td>\n",
       "      <td>220.30</td>\n",
       "      <td>3.263704</td>\n",
       "      <td>0.871443</td>\n",
       "      <td>...</td>\n",
       "      <td>90.13951</td>\n",
       "      <td>180.98</td>\n",
       "      <td>181.33</td>\n",
       "      <td>0.437050</td>\n",
       "      <td>0.772115</td>\n",
       "      <td>0.827564</td>\n",
       "      <td>0.772115</td>\n",
       "      <td>0.055448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14309500</th>\n",
       "      <td>4.977781</td>\n",
       "      <td>3.122204</td>\n",
       "      <td>-0.995847</td>\n",
       "      <td>0.061255</td>\n",
       "      <td>0.627228</td>\n",
       "      <td>15.10</td>\n",
       "      <td>1.776471</td>\n",
       "      <td>222.65</td>\n",
       "      <td>6.893189</td>\n",
       "      <td>0.863563</td>\n",
       "      <td>...</td>\n",
       "      <td>110.42527</td>\n",
       "      <td>224.92</td>\n",
       "      <td>226.31</td>\n",
       "      <td>0.459455</td>\n",
       "      <td>0.885777</td>\n",
       "      <td>0.918714</td>\n",
       "      <td>0.885777</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.032937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14316700</th>\n",
       "      <td>4.543400</td>\n",
       "      <td>2.277630</td>\n",
       "      <td>-0.821172</td>\n",
       "      <td>0.176337</td>\n",
       "      <td>0.501305</td>\n",
       "      <td>14.75</td>\n",
       "      <td>1.446078</td>\n",
       "      <td>214.85</td>\n",
       "      <td>6.018207</td>\n",
       "      <td>0.444680</td>\n",
       "      <td>...</td>\n",
       "      <td>119.08920</td>\n",
       "      <td>587.90</td>\n",
       "      <td>588.01</td>\n",
       "      <td>0.508616</td>\n",
       "      <td>0.911744</td>\n",
       "      <td>0.950213</td>\n",
       "      <td>0.911744</td>\n",
       "      <td>0.038469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.038469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14325000</th>\n",
       "      <td>6.297437</td>\n",
       "      <td>2.434652</td>\n",
       "      <td>-0.952055</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.386610</td>\n",
       "      <td>14.60</td>\n",
       "      <td>1.467337</td>\n",
       "      <td>219.05</td>\n",
       "      <td>6.240741</td>\n",
       "      <td>0.903702</td>\n",
       "      <td>...</td>\n",
       "      <td>124.96889</td>\n",
       "      <td>443.07</td>\n",
       "      <td>444.92</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.928631</td>\n",
       "      <td>0.947802</td>\n",
       "      <td>0.928631</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.019170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14362250</th>\n",
       "      <td>2.781676</td>\n",
       "      <td>3.325188</td>\n",
       "      <td>-0.985486</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>1.195390</td>\n",
       "      <td>20.45</td>\n",
       "      <td>1.786026</td>\n",
       "      <td>260.35</td>\n",
       "      <td>7.354520</td>\n",
       "      <td>0.983481</td>\n",
       "      <td>...</td>\n",
       "      <td>109.93127</td>\n",
       "      <td>41.42</td>\n",
       "      <td>43.88</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>0.906987</td>\n",
       "      <td>0.934313</td>\n",
       "      <td>0.906987</td>\n",
       "      <td>0.027326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14400000</th>\n",
       "      <td>5.556071</td>\n",
       "      <td>2.279668</td>\n",
       "      <td>-1.015946</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.410302</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1.537849</td>\n",
       "      <td>237.00</td>\n",
       "      <td>6.909621</td>\n",
       "      <td>0.606239</td>\n",
       "      <td>...</td>\n",
       "      <td>98.81802</td>\n",
       "      <td>702.63</td>\n",
       "      <td>703.37</td>\n",
       "      <td>0.485781</td>\n",
       "      <td>0.897190</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.897190</td>\n",
       "      <td>0.037236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.037236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                p_mean  pet_mean  p_seasonality  frac_snow   aridity  \\\n",
       "basin_id_str                                                           \n",
       "01022500      3.608126  2.119256      -0.114530   0.245259  0.587356   \n",
       "01031500      3.522957  2.071324       0.104091   0.291836  0.587950   \n",
       "01047000      3.323146  2.090024       0.147776   0.280118  0.628929   \n",
       "01052500      3.730858  2.096423       0.152097   0.352698  0.561914   \n",
       "01054200      4.067132  2.128355       0.104960   0.299642  0.523306   \n",
       "...                ...       ...            ...        ...       ...   \n",
       "14309500      4.977781  3.122204      -0.995847   0.061255  0.627228   \n",
       "14316700      4.543400  2.277630      -0.821172   0.176337  0.501305   \n",
       "14325000      6.297437  2.434652      -0.952055   0.030203  0.386610   \n",
       "14362250      2.781676  3.325188      -0.985486   0.141500  1.195390   \n",
       "14400000      5.556071  2.279668      -1.015946   0.024330  0.410302   \n",
       "\n",
       "              high_prec_freq  high_prec_dur  low_prec_freq  low_prec_dur  \\\n",
       "basin_id_str                                                               \n",
       "01022500               20.55       1.205279         233.65      3.662226   \n",
       "01031500               18.90       1.148936         227.35      3.473644   \n",
       "01047000               20.10       1.165217         235.90      3.691706   \n",
       "01052500               13.50       1.129707         193.50      2.896707   \n",
       "01054200               17.50       1.194539         220.30      3.263704   \n",
       "...                      ...            ...            ...           ...   \n",
       "14309500               15.10       1.776471         222.65      6.893189   \n",
       "14316700               14.75       1.446078         214.85      6.018207   \n",
       "14325000               14.60       1.467337         219.05      6.240741   \n",
       "14362250               20.45       1.786026         260.35      7.354520   \n",
       "14400000               19.30       1.537849         237.00      6.909621   \n",
       "\n",
       "              glim_1st_class_frac  ...  slope_mean  area_gages2  \\\n",
       "basin_id_str                       ...                            \n",
       "01022500                 0.590658  ...    17.79072       573.60   \n",
       "01031500                 0.448928  ...    29.56035       769.05   \n",
       "01047000                 0.308488  ...    49.92122       909.10   \n",
       "01052500                 0.497458  ...    60.05183       383.82   \n",
       "01054200                 0.871443  ...    90.13951       180.98   \n",
       "...                           ...  ...         ...          ...   \n",
       "14309500                 0.863563  ...   110.42527       224.92   \n",
       "14316700                 0.444680  ...   119.08920       587.90   \n",
       "14325000                 0.903702  ...   124.96889       443.07   \n",
       "14362250                 0.983481  ...   109.93127        41.42   \n",
       "14400000                 0.606239  ...    98.81802       702.63   \n",
       "\n",
       "              area_geospa_fabric  baseflow_index   nse_sim    nse_ar  \\\n",
       "basin_id_str                                                           \n",
       "01022500                  620.38        0.554478  0.858931  0.960366   \n",
       "01031500                  766.53        0.445091  0.896980  0.950614   \n",
       "01047000                  904.94        0.473465  0.859059  0.919627   \n",
       "01052500                  396.10        0.459700  0.871757  0.928358   \n",
       "01054200                  181.33        0.437050  0.772115  0.827564   \n",
       "...                          ...             ...       ...       ...   \n",
       "14309500                  226.31        0.459455  0.885777  0.918714   \n",
       "14316700                  588.01        0.508616  0.911744  0.950213   \n",
       "14325000                  444.92        0.480769  0.928631  0.947802   \n",
       "14362250                   43.88        0.518408  0.906987  0.934313   \n",
       "14400000                  703.37        0.485781  0.897190  0.934426   \n",
       "\n",
       "                nse_da  nse_ar-sim  nse_da-sim  nse_da-ar  \n",
       "basin_id_str                                               \n",
       "01022500      0.858931    0.101435         0.0  -0.101435  \n",
       "01031500      0.896980    0.053634         0.0  -0.053634  \n",
       "01047000      0.859059    0.060569         0.0  -0.060569  \n",
       "01052500      0.871757    0.056601         0.0  -0.056601  \n",
       "01054200      0.772115    0.055448         0.0  -0.055448  \n",
       "...                ...         ...         ...        ...  \n",
       "14309500      0.885777    0.032937         0.0  -0.032937  \n",
       "14316700      0.911744    0.038469         0.0  -0.038469  \n",
       "14325000      0.928631    0.019170         0.0  -0.019170  \n",
       "14362250      0.906987    0.027326         0.0  -0.027326  \n",
       "14400000      0.897190    0.037236         0.0  -0.037236  \n",
       "\n",
       "[531 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-linear Regression in a K-fold loop\n",
    "def cross_val_lr(X,y,feature_list,kf,ynormal=False, title='scatterplottitle', corr_matrix=None):\n",
    "\n",
    "    if len(feature_list_short) > 1:\n",
    "        index_labs = feature_list_short\n",
    "    else:\n",
    "        index_labs = feature_list\n",
    "    feature_importances = pd.DataFrame(index=index_labs, columns=['total_importance'])\n",
    "    for iF, F in enumerate(feature_importances.index.values):\n",
    "        feature_importances.loc[F, 'total_importance'] = 0\n",
    "    \n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "    y_hat_list = []\n",
    "    y_test_list = []\n",
    "    y_hat_indx_list = []\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6.5,8.04))\n",
    "    grid = plt.GridSpec(2, 3, wspace=0.33333)\n",
    "    plt.subplot(grid[0, :2])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "                \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_hat = lr.predict(X_test)\n",
    "\n",
    "        # Get numerical feature importances\n",
    "        importances = list(lr.coef_)\n",
    "        for iF, F in enumerate(feature_importances.index.values):\n",
    "            feature_importances.loc[F, 'total_importance'] += importances[iF]\n",
    "        \n",
    "        rmse = np.sqrt(np.mean(np.power((y_test - y_hat),2)))\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(y_test, y_hat)\n",
    "        r2 = r_value**2        \n",
    "#        plt.scatter(y_test,y_hat, c='b')\n",
    "        \n",
    "        rmse_list.append(rmse)\n",
    "        r2_list.append(r2)\n",
    "        y_hat_list.extend(y_hat)\n",
    "        y_test_list.extend(y_test)\n",
    "        y_hat_indx_list.extend(test_index)\n",
    "    \n",
    "    feature_importances.loc[:, 'total_importance'] = feature_importances.loc[:, 'total_importance'] / \\\n",
    "                                                     kf.get_n_splits()\n",
    "\n",
    "\n",
    "    y_hat_all = sort_list(y_hat_list, y_hat_indx_list)\n",
    "    \n",
    "    #############       Positive or Negative color coded importances\n",
    "    if corr_matrix is not None:\n",
    "        bar_colorz=[]\n",
    "        bar_labelz=[]\n",
    "        for feature_importance in feature_importances.index.values:\n",
    "            if corr_matrix.loc[feature_importance]>0:\n",
    "                bar_colorz.append('b')\n",
    "                bar_labelz.append('positive')\n",
    "            elif corr_matrix.loc[feature_importance]<0:\n",
    "                bar_colorz.append('r')\n",
    "                bar_labelz.append('negative')\n",
    "            else:\n",
    "                bar_colorz.append('black')\n",
    "                bar_labelz.append('noncorrelated')\n",
    "    else:\n",
    "        #############        Do a regular linear regression, so we know if the correlation is Positive or Negative\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        linear_coefficients = lr.coef_\n",
    "        print(lr.predict)\n",
    "        slope, intercept, overall_r_value, _, _ = scipy.stats.linregress(y, lr.predict())\n",
    "        print('multi-linear regression r2:', overall_r_value)\n",
    "        bar_colorz=list(np.where(lr.coef_>0, 'b', 'r'))\n",
    "        bar_labelz=list(np.where(lr.coef_>0, 'positive', 'negative'))\n",
    "    feature_importances.loc[:, 'bar_colorz'] = bar_colorz\n",
    "    feature_importances.loc[:, 'bar_labelz'] = bar_labelz\n",
    "    feature_importances = feature_importances.sort_values('total_importance', ascending=False)\n",
    "    #############       Positive or Negative color coded importances\n",
    "    \n",
    "    overall_rmse = np.sqrt(np.mean(np.power((y - y_hat_all),2)))\n",
    "    slope, intercept, overall_r_value, _, _ = scipy.stats.linregress(y, y_hat_all)\n",
    "    overall_r2_value = overall_r_value**2\n",
    "    print('Overall rmse for the out-of-sample predictions: {:.2f}'.format(overall_rmse))\n",
    "    print('Overall r^2 for the out-of-sample predictions: {:.2f}'.format(overall_r_value**2))\n",
    "    print(\"RMSE Value stats for cross validation\")\n",
    "    print_stats(rmse_list)\n",
    "    print(\"R-squared Value stats for cross validation\")\n",
    "    print_stats(r2_list)\n",
    "    x=np.linspace(np.min(y), np.max(y), num=2)\n",
    "    omax = np.max([np.max(y), np.max(y_hat_all)])\n",
    "    omin = np.min([np.min(y), np.min(y_hat_all)])\n",
    "    \n",
    "    plt.scatter(y_test_list,y_hat_list, c='b', label='r2 = {:.2f}'.format(np.nanmean(np.array(r2_list))))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"true target values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    \n",
    "    plt.xlim([omin,omax])\n",
    "    plt.ylim([omin,omax])\n",
    "    plt.plot(x, x*slope+intercept, label='slope:{:.2f}, intercept:{:.2f}'.format(slope,intercept))\n",
    "    plt.plot([omin, omax],[omin, omax],'--',c='k',label='1 to 1 line')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(grid[1, :])\n",
    "    plt.bar(feature_importances.index.values,\n",
    "            feature_importances.total_importance,\n",
    "            color=feature_importances.bar_colorz,\n",
    "            label=feature_importances.bar_labelz)\n",
    "    \n",
    "    # --------------   This is a hack to get the legend to show the color groups #-----#\n",
    "    colors = {'negative correlation':'red', 'positive correlation':'blue'}       #-----#\n",
    "    labels = list(colors.keys())                                                 #-----#\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]#-----#\n",
    "    plt.legend(handles, labels)                                                  #-----#\n",
    "    # --------------   This is a hack to get the legend to show the color groups #-----#\n",
    "\n",
    "    plt.ylabel('feature importance')\n",
    "    plt.xticks(rotation=90)\n",
    "#    plt.grid()\n",
    "\n",
    "    grid.tight_layout(fig)\n",
    "    plt.show()\n",
    "    plt.close()    \n",
    "\n",
    "    return y_hat_all, overall_rmse, overall_r2_value, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression in a K-fold loop\n",
    "def cross_val_rf(X,y,feature_list,kf,ynormal=False,\n",
    "                 n_estimators=10,random_state=42,\n",
    "                 criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                 min_weight_fraction_leaf=0, feature_list_short=[], title='scatterplottitle', filename=None,\n",
    "                 corr_matrix=None):\n",
    "\n",
    "    if len(feature_list_short) > 1:\n",
    "        index_labs = feature_list_short\n",
    "    else:\n",
    "        index_labs = feature_list\n",
    "    feature_importances = pd.DataFrame(index=index_labs, columns=['total_importance'])\n",
    "    for iF, F in enumerate(feature_importances.index.values):\n",
    "        feature_importances.loc[F, 'total_importance'] = 0\n",
    "    \n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "    y_hat_list = []\n",
    "    y_test_list = []\n",
    "    y_hat_indx_list = []\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6.5,8.04))\n",
    "    grid = plt.GridSpec(2, 3, wspace=0.33333)\n",
    "    plt.subplot(grid[0, :2])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "                \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state,\n",
    "                                  criterion=criterion, min_samples_leaf=min_samples_leaf,\n",
    "                                  min_samples_split=min_samples_split, max_depth=max_depth,\n",
    "                                  min_weight_fraction_leaf=min_weight_fraction_leaf)\n",
    "        rf.fit(X_train, y_train);\n",
    "        y_hat = rf.predict(X_test)\n",
    "\n",
    "        # Get numerical feature importances\n",
    "        importances = list(rf.feature_importances_)\n",
    "        for iF, F in enumerate(feature_importances.index.values):\n",
    "            feature_importances.loc[F, 'total_importance'] += importances[iF]\n",
    "        \n",
    "        rmse = np.sqrt(np.mean(np.power((y_test - y_hat),2)))\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(y_test, y_hat)\n",
    "        r2 = r_value**2        \n",
    "#        plt.scatter(y_test,y_hat, c='b')\n",
    "        \n",
    "        rmse_list.append(rmse)\n",
    "        r2_list.append(r2)\n",
    "        y_hat_list.extend(y_hat)\n",
    "        y_test_list.extend(y_test)\n",
    "        y_hat_indx_list.extend(test_index)\n",
    "    \n",
    "    feature_importances.loc[:, 'total_importance'] = feature_importances.loc[:, 'total_importance'] / \\\n",
    "                                                     kf.get_n_splits()\n",
    "\n",
    "    print(feature_importances)\n",
    "    \n",
    "    y_hat_all = sort_list(y_hat_list, y_hat_indx_list)\n",
    "    \n",
    "    #############       Positive or Negative color coded importances\n",
    "    if corr_matrix is not None:\n",
    "        bar_colorz=[]\n",
    "        bar_labelz=[]\n",
    "        for feature_importance in feature_importances.index.values:\n",
    "            if corr_matrix.loc[feature_importance]>0:\n",
    "                bar_colorz.append('b')\n",
    "                bar_labelz.append('positive')\n",
    "            elif corr_matrix.loc[feature_importance]<0:\n",
    "                bar_colorz.append('r')\n",
    "                bar_labelz.append('negative')\n",
    "            else:\n",
    "                bar_colorz.append('black')\n",
    "                bar_labelz.append('noncorrelated')\n",
    "    else:\n",
    "        #############        Do a regular linear regression, so we know if the correlation is Positive or Negative\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        linear_coefficients = lr.coef_\n",
    "        print(lr.predict)\n",
    "        slope, intercept, overall_r_value, _, _ = scipy.stats.linregress(y, lr.predict())\n",
    "        print('multi-linear regression r2:', overall_r_value)\n",
    "        bar_colorz=list(np.where(lr.coef_>0, 'b', 'r'))\n",
    "        bar_labelz=list(np.where(lr.coef_>0, 'positive', 'negative'))\n",
    "    feature_importances.loc[:, 'bar_colorz'] = bar_colorz\n",
    "    feature_importances.loc[:, 'bar_labelz'] = bar_labelz\n",
    "    feature_importances = feature_importances.sort_values('total_importance', ascending=False)\n",
    "    #############       Positive or Negative color coded importances\n",
    "    \n",
    "    overall_rmse = np.sqrt(np.mean(np.power((y - y_hat_all),2)))\n",
    "    slope, intercept, overall_r_value, _, _ = scipy.stats.linregress(y, y_hat_all)\n",
    "    overall_r2_value = overall_r_value**2\n",
    "    print('Overall rmse for the out-of-sample predictions: {:.2f}'.format(overall_rmse))\n",
    "    print('Overall r^2 for the out-of-sample predictions: {:.2f}'.format(overall_r_value**2))\n",
    "    print(\"RMSE Value stats for cross validation\")\n",
    "    print_stats(rmse_list)\n",
    "    print(\"R-squared Value stats for cross validation\")\n",
    "    print_stats(r2_list)\n",
    "    x=np.linspace(np.min(y), np.max(y), num=2)\n",
    "    omax = np.max([np.max(y), np.max(y_hat_all)])\n",
    "    omin = np.min([np.min(y), np.min(y_hat_all)])\n",
    "    \n",
    "    plt.scatter(y_test_list,y_hat_list, c='b', label='r2 = {:.2f}'.format(np.nanmean(np.array(r2_list))))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"true target values\")\n",
    "    plt.ylabel(\"predicted values\")\n",
    "    \n",
    "    plt.xlim([omin,omax])\n",
    "    plt.ylim([omin,omax])\n",
    "    plt.plot(x, x*slope+intercept, label='slope:{:.2f}, intercept:{:.2f}'.format(slope,intercept))\n",
    "    plt.plot([omin, omax],[omin, omax],'--',c='k',label='1 to 1 line')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(grid[1, :])\n",
    "    plt.bar(feature_importances.index.values,\n",
    "            feature_importances.total_importance,\n",
    "            color=feature_importances.bar_colorz,\n",
    "            label=feature_importances.bar_labelz)\n",
    "    \n",
    "    # --------------   This is a hack to get the legend to show the color groups #-----#\n",
    "    colors = {'negative correlation':'red', 'positive correlation':'blue'}       #-----#\n",
    "    labels = list(colors.keys())                                                 #-----#\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]#-----#\n",
    "    plt.legend(handles, labels)                                                  #-----#\n",
    "    # --------------   This is a hack to get the legend to show the color groups #-----#\n",
    "\n",
    "    plt.ylabel('feature importance')\n",
    "    plt.xticks(rotation=90)\n",
    "#    plt.grid()\n",
    "\n",
    "    grid.tight_layout(fig)\n",
    "    plt.show()\n",
    "    if filename:\n",
    "        fig.savefig(filename+'.pdf', dpi=300)\n",
    "        fig.savefig(filename+'.png', dpi=300)\n",
    "    plt.close()    \n",
    "\n",
    "    return y_hat_all, overall_rmse, overall_r2_value, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training and targets.\n",
    "def set_X_y(df, regressor_attributes, target, xnormal=False, ynormal=False, xlog=False, ylog=False):\n",
    "    X=np.array(df.loc[:,regressor_attributes])\n",
    "    y=np.array(df[target])\n",
    "    X_krig=np.array(df.loc[:,['gauge_lat', 'gauge_lon']])\n",
    "\n",
    "    if ynormal:\n",
    "        y_mean = np.nanmean(y)\n",
    "        y_std = np.nanstd(y)\n",
    "        y = (y-y_mean)/y_std\n",
    "    \n",
    "    if xnormal:\n",
    "        for i, r in enumerate(regressor_attributes):\n",
    "            iX = np.array(df.loc[:,r])\n",
    "            X_mean = np.nanmean(iX)\n",
    "            X_std = np.nanstd(iX)\n",
    "            X[:,i] = (iX - X_mean) / X_std\n",
    "    \n",
    "    if xlog:\n",
    "        for i, r in enumerate(regressor_attributes):\n",
    "            for iX in range(X.shape[0]):\n",
    "                if np.abs(np.log(np.array(df.iloc[iX,i]))) > 0:\n",
    "                    X[iX] = np.log(np.array(df.iloc[iX,i]))\n",
    "    if ylog:\n",
    "        for iy in range(y.shape[0]):\n",
    "            if np.abs(np.log(y[iy])) > 0:\n",
    "                y[iy] = np.log(y[iy])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list(list1, list2): \n",
    "    zipped_pairs = zip(list2, list1) \n",
    "    z = [x for _, x in sorted(zipped_pairs)] \n",
    "    return z\n",
    "def print_stats(d):\n",
    "    print(\"mean = {:.2f},  median = {:.2f}, stdev = {:.2f}, min = {:.2f}, max = {:.2f}\".format(\n",
    "        np.nanmean(np.array(d)),\n",
    "        np.nanmedian(np.array(d)),\n",
    "        np.nanstd(np.array(d)),\n",
    "        np.nanmin(np.array(d)),\n",
    "        np.nanmax(np.array(d))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr().iloc[:,-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "\n",
    "target='nse_da-ar'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE diff: Data Assimilation - Autoregression', \n",
    "                filename='corr_diff_da_ar', corr_matrix=corr_matrix[target])\n",
    "\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                  title='Linear regression, NSE diff: DA - AR',\n",
    "                                                   corr_matrix=corr_matrix[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "\n",
    "target='nse_ar-sim'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE diff: Autoregression - Simulation',\n",
    "                filename='corr_diff_ar_sim', corr_matrix=corr_matrix[target])\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                   title='Linear regression, NSE diff: AR - Sim', \n",
    "                                                   corr_matrix=corr_matrix[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "\n",
    "target='nse_da-sim'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE diff: Data Assimilation - Simulation',\n",
    "                filename='corr_diff_da_sim', corr_matrix=corr_matrix[target])\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                   title='Linear regression, NSE diff: DA - Sim', \n",
    "                                                   corr_matrix=corr_matrix[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for all the X, y values\n",
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "feature_list_short = []\n",
    "target='nse_sim'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE: Simulation',\n",
    "                filename='corr_simulation', corr_matrix=corr_matrix[target])\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                   title='Linear regression, Simulation', \n",
    "                                                   corr_matrix=corr_matrix[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for all the X, y values\n",
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "\n",
    "feature_list_short = []\n",
    "target='nse_da'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE: Data Assimilation',\n",
    "                filename='corr_data_assimilation', corr_matrix=corr_matrix[target])\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                   title='Linear regression, Data Assimilation', \n",
    "                                                   corr_matrix=corr_matrix[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for all the X, y values\n",
    "regressor_attributes = static_attributes#df.columns.values[:-6]\n",
    "\n",
    "feature_list_short = []\n",
    "target='nse_ar'\n",
    "ynormal = True\n",
    "X, y = set_X_y(df, regressor_attributes, target, xnormal=True, ynormal=ynormal)\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True)\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,\n",
    "                ynormal=ynormal, n_estimators=1000,random_state=100,\n",
    "                criterion='mse', min_samples_leaf=1, min_samples_split=2, max_depth=None,\n",
    "                min_weight_fraction_leaf=0, title='NSE: Autoregression',\n",
    "                filename='corr_autoregression', corr_matrix=corr_matrix[target])\n",
    "y_hat, rmse, r2, feature_importance = cross_val_rf(X,y,regressor_attributes,kf,ynormal=ynormal,\n",
    "                                                   title='Linear regression, Autoregression', \n",
    "                                                   corr_matrix=corr_matrix[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model_label_map = {'nse_da':'Data Assimilation', 'nse_ar':'Autoregression'}\n",
    "\n",
    "for test_model in ['nse_da', 'nse_ar']:\n",
    "    \n",
    "    gs = plt.GridSpec(1, 10, wspace=0.33333)\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax0 = fig.add_subplot(gs[0,:6])\n",
    "    im = ax0.scatter(plot_lons, plot_lats,\n",
    "                    c=df.loc[:,test_model+'-sim'],\n",
    "                    s=20,\n",
    "                    cmap='PRGn',\n",
    "                    vmin=-.1, vmax=.1)\n",
    "    ax0.set_title('NSE difference: {} - Simulation'.format(test_model_label_map[test_model]))\n",
    "    clims = im.get_clim()\n",
    "\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax0)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0,7:])\n",
    "    ax1.plot([0, 1],[0,1], 'k--', lw=0.6)\n",
    "    for b, basin in enumerate(basin_list):\n",
    "        basin_color = im.to_rgba(df.loc[:,test_model+'-sim'])\n",
    "        ax1.scatter(df.loc[:,'nse_sim'],\n",
    "                    df.loc[:,test_model],\n",
    "                    s=5,\n",
    "                    color=basin_color)\n",
    "    ax1.set_title('NSE')\n",
    "    ax1.set_xlabel('Simulation')\n",
    "    ax1.set_ylabel(test_model_label_map[test_model])\n",
    "    ax1.set_xlim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('./figs/map_'+test_model+'.pdf', dpi=300)\n",
    "    fig.savefig('./figs/map_'+test_model+'.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label_map = {'nse_da':'Data Assimilation', 'nse_ar':'Autoregression', 'nse_sim':'Simulation'}\n",
    "for model in ['nse_sim', 'nse_ar', 'nse_da']:\n",
    "    gs = plt.GridSpec(1, 10, wspace=0.33333)\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax0 = fig.add_subplot(gs[0,:6])\n",
    "    im = ax0.scatter(plot_lons, plot_lats,\n",
    "                    c=df.loc[:,model],\n",
    "                    s=20,\n",
    "                    cmap='PRGn',\n",
    "                    vmin=-1, vmax=1)\n",
    "    ax0.set_title('NSE: {}'.format(model_label_map[model]))\n",
    "    clims = im.get_clim()\n",
    "    \n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax0)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_label_map = {'nse_da':'Data Assimilation', 'nse_ar':'Autoregression'}\n",
    "\n",
    "for plot_variable in ['high_prec_freq', 'p_mean']:\n",
    "    \n",
    "    gs = plt.GridSpec(1, 10, wspace=0.33333)\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax0 = fig.add_subplot(gs[0,:6])\n",
    "    im = ax0.scatter(plot_lons, plot_lats,\n",
    "                    c=df.loc[:,plot_variable],\n",
    "                    s=20,\n",
    "                    cmap='PRGn',\n",
    "                    vmin=np.min(df.loc[:,plot_variable]), vmax=np.max(df.loc[:,plot_variable]))\n",
    "    ax0.set_title(plot_variable)\n",
    "    clims = im.get_clim()\n",
    "\n",
    "    # colorbar\n",
    "    divider = make_axes_locatable(ax0)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0,7:])\n",
    "    ax1.plot([0, 1],[0,1], 'k--', lw=0.6)\n",
    "    for b, basin in enumerate(basin_list):\n",
    "        basin_color = im.to_rgba(df.loc[:,plot_variable])\n",
    "        ax1.scatter(df.loc[:,'nse_da-sim'],\n",
    "                    df.loc[:,plot_variable],\n",
    "                    s=5,\n",
    "                    color=basin_color)\n",
    "    ax1.set_title('NSE')\n",
    "    ax1.set_xlabel('DA-Simulation')\n",
    "    ax1.set_ylabel(plot_variable)\n",
    "    ax1.set_xlim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
